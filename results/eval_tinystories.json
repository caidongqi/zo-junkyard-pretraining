{
  "checkpoint_path": "/data/cdq/current_project/zo-test-cdq/logs/parallel_sweep_20251105_111831/experiments/FO_full_bs2_qN_A_bpN_A_optmudamw_lr1e-3/logs/parallel_sweep_20251105_111831/experiments/FO_full_bs2_qN_A_bpN_A_optmudamw_lr1e-3/checkpoint",
  "dataset": "tinystories",
  "batch_size": 16,
  "block_size": 128,
  "max_samples": 5000,
  "max_batches": 100,
  "device": "cuda",
  "avg_loss": 4.6546257209777835,
  "perplexity": 105.06990814208984,
  "total_tokens": 204800,
  "model_config": {
    "activation_function": "gelu_new",
    "architectures": [
      "GPT2LMHeadModel"
    ],
    "attn_pdrop": 0.1,
    "bos_token_id": 50257,
    "dtype": "float32",
    "embd_pdrop": 0.1,
    "eos_token_id": 50257,
    "initializer_range": 0.02,
    "layer_norm_epsilon": 1e-05,
    "model_type": "gpt2",
    "n_embd": 256,
    "n_head": 4,
    "n_inner": null,
    "n_layer": 6,
    "n_positions": 512,
    "reorder_and_upcast_attn": false,
    "resid_pdrop": 0.1,
    "scale_attn_by_inverse_layer_idx": false,
    "scale_attn_weights": true,
    "summary_activation": null,
    "summary_first_dropout": 0.1,
    "summary_proj_to_labels": true,
    "summary_type": "cls_index",
    "summary_use_proj": true,
    "transformers_version": "4.57.1",
    "use_cache": true,
    "vocab_size": 50258
  },
  "timestamp": "2025-11-05T13:40:47.509362"
}