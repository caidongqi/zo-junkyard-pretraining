#!/bin/bash

# Parallel ZO vs FO Parameter Sweep Script
# æ”¯æŒå¹¶è¡Œè¿è¡Œå’ŒGPUé€‰æ‹©çš„å‚æ•°æ‰«æè„šæœ¬

# ä¸è¦ä½¿ç”¨ set -eï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å¤„ç†é”™è¯¯
# set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m' # No Color

# å…¨å±€å˜é‡ï¼šå­˜å‚¨æ‰€æœ‰å­è¿›ç¨‹ PID
declare -a ALL_CHILD_PIDS=()
MAIN_PID=$$
PID_FILE=""
CLEANUP_DONE=false

# é»˜è®¤é…ç½®å‚æ•°
MODES=("FO" "Instruct") # å¯é€‰: FO, ZO, Calibrate, Instruct
SCOPES=("full")
BATCH_SIZES=(8)
BLOCK_SIZES=(512)  # åºåˆ—é•¿åº¦ (å¯é€‰: 64, 128, 256, 512, 1024)
QUERY_BUDGETS=(8)
BP_INTERVALS=(1)
INSTRUCT_COSINE_TARGETS=(0.01)
INSTRUCT_NOISE_SCALES=(10.0)
LEARNING_RATES_ZO=(1e-3)
OPTIMIZERS=("mudamw")  # å¯é€‰: sgd, adam, mudamw
EPOCHS=10
LOG_INTERVAL=10

# å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½® (Learning Rate Scheduler Configuration)
USE_LR_SCHEDULER=true  # æ˜¯å¦å¯ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨
WARMUP_STEPS=300       # é¢„çƒ­æ­¥æ•°
MIN_LR=1e-6           # æœ€å°å­¦ä¹ ç‡

# æ¢¯åº¦ç´¯ç§¯é…ç½® (Gradient Accumulation Configuration)
# ä»…é€‚ç”¨äºFOæ¨¡å¼ã€‚æœ‰æ•ˆbatch size = batch_size * gradient_accumulation_steps
GRADIENT_ACCUMULATION_STEPS=8  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œ1è¡¨ç¤ºä¸ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯

LOGS_ROOT="logs"

# æ¨¡å‹é…ç½® (Model Configuration)
# å¤‡é€‰: 20M (è¶…å°å‹ï¼Œå¿«é€Ÿå®éªŒ), 200M (ä¸­å°å‹ï¼Œç±»ä¼¼GPT-2 Small), 500M (ä¸­å‹), 1B (å¤§å‹)
MODEL_SIZES=("20M")  # é»˜è®¤ä½¿ç”¨200Mæ¨¡å‹ï¼Œå¯ä»¥æ˜¯æ•°ç»„å¦‚: ("20M" "200M" "500M" "1B")

# æ•°æ®é›†é…ç½® (Dataset Configuration)
# å¤‡é€‰æ•°æ®é›†:
#   - cosmopedia-100k: é«˜è´¨é‡åˆæˆæ•™è‚²æ•°æ®ï¼Œ100kæ ·æœ¬ï¼Œå¿«é€Ÿå®éªŒ (æ¨èç”¨äºå¿«é€Ÿæµ‹è¯•)
#   - cosmopedia: Cosmopediaå®Œæ•´ç‰ˆï¼Œ30M+æ ·æœ¬ï¼Œé«˜è´¨é‡
#   - wikitext-103: ç»´åŸºç™¾ç§‘æ–‡æœ¬ï¼Œç»å…¸é¢„è®­ç»ƒæ•°æ®é›†
#   - openwebtext: å¼€æºWebTextï¼Œ8M+ç½‘é¡µæ–‡æ¡£ï¼Œæ¥è¿‘çœŸå®åˆ†å¸ƒ
#   - c4: è¶…å¤§è§„æ¨¡æ¸…æ´—ç½‘é¡µæ•°æ®ï¼Œ365Mæ–‡æ¡£ï¼Œé€‚åˆå¤§è§„æ¨¡é¢„è®­ç»ƒ
#   - tinystories: ç®€å•æ•…äº‹æ•°æ®é›†ï¼Œé€‚åˆå°æ¨¡å‹è°ƒè¯•
#   - pile-subset: The Pileæ— ç‰ˆæƒå­é›†ï¼Œå¤šæ ·åŒ–é«˜è´¨é‡æ•°æ®
#   - fineweb: FineWebå®Œæ•´ç‰ˆï¼Œ15T tokensï¼Œä¸»æµé«˜è´¨é‡é¢„è®­ç»ƒæ•°æ® (æ¨èç”¨äºæ­£å¼è®­ç»ƒ)
#   - fineweb-edu: FineWebæ•™è‚²å­é›†ï¼Œ1.3T tokensï¼Œé«˜è´¨é‡æ¨è
#   - fineweb-edu-10bt: FineWeb-Edu 10BTé‡‡æ ·ï¼Œé€‚åˆå¿«é€Ÿå®éªŒ
DATASET="fineweb-edu-10bt"  # é»˜è®¤ä½¿ç”¨cosmopedia-100k (å¿«é€Ÿæµ‹è¯•æ¨èfineweb-edu-10bt)

# æ•°æ®é›†æœ€å¤§æ ·æœ¬æ•° (Dataset Max Samples)
# è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä½¿ç”¨æ•°æ®é›†çš„æ¨èå€¼
# å»ºè®®å€¼å‚è€ƒ:
#   cosmopedia-100k(20000), cosmopedia(100000), openwebtext(50000), c4(100000)
#   fineweb(100000), fineweb-edu(50000), fineweb-edu-10bt(30000)
MAX_SAMPLES=""  # ç•™ç©ºä½¿ç”¨æ¨èå€¼ï¼Œæˆ–æŒ‡å®šå…·ä½“æ•°å­—å¦‚: 20000

# BPæ•°æ®é›†é…ç½® (BP Dataset Configuration for Calibrate/Instruct modes)
# ç”¨äºCalibrate/Instructæ¨¡å¼ä¸­BPæ¢¯åº¦è®¡ç®—çš„æ•°æ®é›†
# ç•™ç©ºè¡¨ç¤ºä½¿ç”¨ä¸ä¸»è®­ç»ƒç›¸åŒçš„æ•°æ®é›†
BP_DATASET="fineweb-edu-10bt"  # ç•™ç©ºä½¿ç”¨ä¸»æ•°æ®é›†ï¼Œæˆ–æŒ‡å®šä¸åŒçš„æ•°æ®é›†å¦‚: "cosmopedia-100k"
BP_MAX_SAMPLES=""  # ç•™ç©ºä½¿ç”¨æ¨èå€¼ï¼Œæˆ–æŒ‡å®šå…·ä½“æ•°å­—

# å¹¶è¡Œé…ç½®
MAX_PARALLEL_JOBS=32 # æœ€å¤§å¹¶è¡Œä»»åŠ¡æ•°
GPU_IDS="2"           # GPU IDåˆ—è¡¨ï¼Œç©ºè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹

# è§£æå‘½ä»¤è¡Œå‚æ•°
while [[ $# -gt 0 ]]; do
    case $1 in
        --parallel)
            MAX_PARALLEL_JOBS="$2"
            shift 2
            ;;
        --gpus)
            GPU_IDS="$2"
            shift 2
            ;;
        --modes)
            IFS=',' read -ra MODES <<< "$2"
            shift 2
            ;;
        --scopes)
            IFS=',' read -ra SCOPES <<< "$2"
            shift 2
            ;;
        --batch-sizes)
            IFS=',' read -ra BATCH_SIZES <<< "$2"
            shift 2
            ;;
        --block-sizes)
            IFS=',' read -ra BLOCK_SIZES <<< "$2"
            shift 2
            ;;
        --query-budgets)
            IFS=',' read -ra QUERY_BUDGETS <<< "$2"
            shift 2
            ;;
        --bp-intervals)
            IFS=',' read -ra BP_INTERVALS <<< "$2"
            shift 2
            ;;
        --learning-rates)
            IFS=',' read -ra LEARNING_RATES_ZO <<< "$2"
            shift 2
            ;;
        --optimizers)
            IFS=',' read -ra OPTIMIZERS <<< "$2"
            shift 2
            ;;
        --epochs)
            EPOCHS="$2"
            shift 2
            ;;
        --log-interval)
            LOG_INTERVAL="$2"
            shift 2
            ;;
        --model-size|--model-sizes)
            IFS=',' read -ra MODEL_SIZES <<< "$2"
            shift 2
            ;;
        --dataset)
            DATASET="$2"
            shift 2
            ;;
        --max-samples)
            MAX_SAMPLES="$2"
            shift 2
            ;;
        --bp-dataset)
            BP_DATASET="$2"
            shift 2
            ;;
        --bp-max-samples)
            BP_MAX_SAMPLES="$2"
            shift 2
            ;;
        --instruct-cosine-targets)
            IFS=',' read -ra INSTRUCT_COSINE_TARGETS <<< "$2"
            shift 2
            ;;
        --instruct-noise-scales)
            IFS=',' read -ra INSTRUCT_NOISE_SCALES <<< "$2"
            shift 2
            ;;
        --use-lr-scheduler)
            USE_LR_SCHEDULER=true
            shift 1
            ;;
        --no-lr-scheduler)
            USE_LR_SCHEDULER=false
            shift 1
            ;;
        --warmup-steps)
            WARMUP_STEPS="$2"
            shift 2
            ;;
        --min-lr)
            MIN_LR="$2"
            shift 2
            ;;
        --gradient-accumulation-steps)
            GRADIENT_ACCUMULATION_STEPS="$2"
            shift 2
            ;;
        -h|--help)
            echo "Usage: $0 [OPTIONS]"
            echo "Options:"
            echo "  --parallel N         æœ€å¤§å¹¶è¡Œä»»åŠ¡æ•° (é»˜è®¤: 32)"
            echo "  --gpus '0,1,2'      æŒ‡å®šGPU IDåˆ—è¡¨ï¼Œæ”¯æŒé€—å·æˆ–ç©ºæ ¼åˆ†éš” (é»˜è®¤: è‡ªåŠ¨æ£€æµ‹)"
            echo "  --modes 'FO,ZO,Calibrate,Instruct'     ä¼˜åŒ–æ–¹æ³• (é»˜è®¤: ZO)"
            echo "  --scopes 'reduced,full' è®­ç»ƒèŒƒå›´ (é»˜è®¤: full)"
            echo "  --batch-sizes '1,2,4' æ‰¹æ¬¡å¤§å° (é»˜è®¤: 2)"
            echo "  --block-sizes '64,128,256' åºåˆ—é•¿åº¦/å—å¤§å° (é»˜è®¤: 128)"
            echo "  --query-budgets '1,2,4,8' Query budget (é»˜è®¤: 1,2,4,...,1024)"
            echo "  --bp-intervals '1,2,5,10'  Calibrate/Instruct æ¨¡å¼çš„BPé—´éš” (é»˜è®¤: 1,2,5,10)"
            echo "  --learning-rates '1e-3,1e-4' å­¦ä¹ ç‡ (é»˜è®¤: 1e-3)"
            echo "  --optimizers 'sgd,adam,mudamw' ä¼˜åŒ–å™¨ (é»˜è®¤: mudamw)"
            echo "  --epochs N           è®­ç»ƒè½®æ•° (é»˜è®¤: 10)"
            echo "  --log-interval N     æ—¥å¿—é—´éš” (é»˜è®¤: 10)"
            echo "  --model-size(s) SIZE æ¨¡å‹å¤§å°ï¼Œæ”¯æŒå¤šä¸ª: '20M,200M,500M,1B' (é»˜è®¤: 200M)"
            echo "  --dataset NAME       æ•°æ®é›†åç§° (é»˜è®¤: cosmopedia-100k)"
            echo "                       å¯é€‰: cosmopedia-100k, cosmopedia, wikitext-103,"
            echo "                             openwebtext, c4, tinystories, pile-subset,"
            echo "                             fineweb, fineweb-edu, fineweb-edu-10bt"
            echo "  --max-samples N      æœ€å¤§æ ·æœ¬æ•°ï¼Œç•™ç©ºä½¿ç”¨æ¨èå€¼ (é»˜è®¤: ä½¿ç”¨æ¨èå€¼)"
            echo "  --bp-dataset NAME    BPæ•°æ®é›†åç§° (Calibrate/Instructæ¨¡å¼ç”¨)"
            echo "                       ç•™ç©ºä½¿ç”¨ä¸»æ•°æ®é›† (é»˜è®¤: ä½¿ç”¨ä¸»æ•°æ®é›†)"
            echo "  --bp-max-samples N   BPæ•°æ®é›†æœ€å¤§æ ·æœ¬æ•° (é»˜è®¤: ä½¿ç”¨æ¨èå€¼)"
            echo "  --instruct-cosine-targets '0.9,0.95'   Instructæ¨¡å¼çš„ä½™å¼¦ç›®æ ‡ (é»˜è®¤: 0.9)"
            echo "  --instruct-noise-scales '0.5,1.0'      Instructæ¨¡å¼çš„å™ªå£°å¼ºåº¦ (é»˜è®¤: 0.5)"
            echo "  --use-lr-scheduler   å¯ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨ (é»˜è®¤: å¯ç”¨)"
            echo "  --no-lr-scheduler    ç¦ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨"
            echo "  --warmup-steps N     å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•° (é»˜è®¤: 300)"
            echo "  --min-lr VALUE       æœ€å°å­¦ä¹ ç‡ (é»˜è®¤: 1e-6)"
            echo "  --gradient-accumulation-steps N  æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (ä»…FOæ¨¡å¼, é»˜è®¤: 1)"
            echo "                       æœ‰æ•ˆbatch size = batch_size * gradient_accumulation_steps"
            echo "  -h, --help           æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"
            exit 0
            ;;
        *)
            echo "Unknown option: $1"
            echo "Use --help for usage information"
            exit 1
            ;;
    esac
done

# è‡ªåŠ¨æ£€æµ‹GPU
if [ -z "$GPU_IDS" ]; then
    if command -v nvidia-smi &> /dev/null; then
        GPU_COUNT=$(nvidia-smi --list-gpus | wc -l)
        if [ $GPU_COUNT -gt 0 ]; then
            GPU_IDS=$(seq -s, 0 $((GPU_COUNT-1)))
            echo -e "${BLUE}ğŸ” Auto-detected $GPU_COUNT GPU(s): $GPU_IDS${NC}"
        else
            echo -e "${YELLOW}âš ï¸  No GPUs detected, using CPU${NC}"
            GPU_IDS="cpu"
        fi
    else
        echo -e "${YELLOW}âš ï¸  nvidia-smi not found, using CPU${NC}"
        GPU_IDS="cpu"
    fi
fi

# åˆ›å»ºæ—¥å¿—ä¸ç»“æœç›®å½•
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
RUN_DESCRIPTOR="${MODES}_${SCOPES}_${BATCH_SIZES}_${QUERY_BUDGETS}_${BP_INTERVALS}_${LEARNING_RATES_ZO}_${OPTIMIZERS}_${EPOCHS}_${LOG_INTERVAL}_${INSTRUCT_COSINE_TARGETS}_${INSTRUCT_NOISE_SCALES}"
mkdir -p "$LOGS_ROOT"
RUN_LOG_ROOT="${LOGS_ROOT}/parallel_sweep_${TIMESTAMP}"
EXPERIMENT_LOG_ROOT="${RUN_LOG_ROOT}/experiments"
RESULTS_DIR="${RUN_LOG_ROOT}/results_${RUN_DESCRIPTOR}"
CSV_DIR="${RUN_LOG_ROOT}/csv_logs_${RUN_DESCRIPTOR}"
CACHE_DIR="cache"
TEMP_DIR="${RUN_LOG_ROOT}/temp"

mkdir -p "$RUN_LOG_ROOT" "$EXPERIMENT_LOG_ROOT" "$RESULTS_DIR" "$CSV_DIR" "$CACHE_DIR" "$TEMP_DIR"

LOG_FILE="${RUN_LOG_ROOT}/parallel_sweep.log"
SUMMARY_FILE="${RUN_LOG_ROOT}/parallel_sweep_summary.txt"
JOB_LOG_DIR="${RUN_LOG_ROOT}/job_logs"
PID_FILE="${RUN_LOG_ROOT}/parallel_sweep.pids"
STATUS_FILE="${RUN_LOG_ROOT}/parallel_sweep.status"
mkdir -p "$JOB_LOG_DIR"

# æ¸…ç†å‡½æ•°
cleanup() {
    if [ "$CLEANUP_DONE" = true ]; then
        return
    fi
    CLEANUP_DONE=true
    
    echo ""
    echo -e "${YELLOW}âš ï¸  æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨æ¸…ç†æ‰€æœ‰å­è¿›ç¨‹...${NC}"
    echo "æ¸…ç†æ—¶é—´: $(date)" >> "$LOG_FILE"
    
    # ä» PID æ–‡ä»¶è¯»å–æ‰€æœ‰å­è¿›ç¨‹
    if [ -f "$PID_FILE" ]; then
        echo "ä» PID æ–‡ä»¶è¯»å–è¿›ç¨‹åˆ—è¡¨: $PID_FILE" >> "$LOG_FILE"
        while IFS= read -r pid; do
            if [ -n "$pid" ] && kill -0 "$pid" 2>/dev/null; then
                echo "ç»ˆæ­¢è¿›ç¨‹: $pid" >> "$LOG_FILE"
                kill -TERM "$pid" 2>/dev/null || true
                ALL_CHILD_PIDS+=("$pid")
            fi
        done < "$PID_FILE"
    fi
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹é€€å‡º
    local wait_count=0
    for pid in "${ALL_CHILD_PIDS[@]}"; do
        if kill -0 "$pid" 2>/dev/null; then
            echo "ç­‰å¾…è¿›ç¨‹ $pid é€€å‡º..." | tee -a "$LOG_FILE"
            wait_count=$((wait_count + 1))
        fi
    done
    
    if [ $wait_count -gt 0 ]; then
        echo "ç­‰å¾… $wait_count ä¸ªè¿›ç¨‹é€€å‡º (æœ€å¤š10ç§’)..." | tee -a "$LOG_FILE"
        sleep 2
        
        # å¼ºåˆ¶ç»ˆæ­¢ä»åœ¨è¿è¡Œçš„è¿›ç¨‹
        for pid in "${ALL_CHILD_PIDS[@]}"; do
            if kill -0 "$pid" 2>/dev/null; then
                echo "å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹: $pid" | tee -a "$LOG_FILE"
                kill -9 "$pid" 2>/dev/null || true
            fi
        done
        sleep 1
    fi
    
    echo -e "${GREEN}âœ… æ¸…ç†å®Œæˆ${NC}"
    echo "æ¸…ç†å®Œæˆæ—¶é—´: $(date)" >> "$LOG_FILE"
    
    # æ›´æ–°çŠ¶æ€æ–‡ä»¶
    if [ -f "$STATUS_FILE" ]; then
        echo "STOPPED_AT=$(date)" >> "$STATUS_FILE"
    fi
}

# æ³¨å†Œä¿¡å·å¤„ç†
trap cleanup EXIT INT TERM QUIT

# åˆå§‹åŒ–çŠ¶æ€æ–‡ä»¶
echo "MAIN_PID=$MAIN_PID" > "$STATUS_FILE"
echo "STARTED_AT=$(date)" >> "$STATUS_FILE"
echo "PID_FILE=$PID_FILE" >> "$STATUS_FILE"
echo "LOG_FILE=$LOG_FILE" >> "$STATUS_FILE"

echo -e "${BLUE}ğŸš€ Starting Parallel ZO vs FO Parameter Sweep${NC}"
echo -e "${BLUE}============================================${NC}"
echo "Max parallel jobs: $MAX_PARALLEL_JOBS"
echo "GPU IDs: $GPU_IDS"
echo "Model sizes: ${MODEL_SIZES[*]}"
echo "Dataset: $DATASET"
if [ -n "$MAX_SAMPLES" ]; then
    echo "Max samples: $MAX_SAMPLES"
else
    echo "Max samples: Using recommended value"
fi
if [ -n "$BP_DATASET" ]; then
    echo "BP Dataset: $BP_DATASET"
    if [ -n "$BP_MAX_SAMPLES" ]; then
        echo "BP Max samples: $BP_MAX_SAMPLES"
    else
        echo "BP Max samples: Using recommended value"
    fi
else
    echo "BP Dataset: Same as main dataset"
fi
echo "Instruct cosine targets: ${INSTRUCT_COSINE_TARGETS[*]}"
echo "Instruct noise scales: ${INSTRUCT_NOISE_SCALES[*]}"
echo "Results will be saved to: $RESULTS_DIR"
echo "CSV logs will be saved to: $CSV_DIR"
echo "Dataset cache: $CACHE_DIR"
echo "Log file: $LOG_FILE"
echo "Run logs directory: $RUN_LOG_ROOT"
echo "Job logs directory: $JOB_LOG_DIR"
echo ""

# ç”Ÿæˆæ‰€æœ‰å®éªŒé…ç½®
generate_experiments() {
    local experiments=()
    local exp_id=0
    
    for model_size in "${MODEL_SIZES[@]}"; do
        for mode in "${MODES[@]}"; do
            for scope in "${SCOPES[@]}"; do
                for batch_size in "${BATCH_SIZES[@]}"; do
                    for block_size in "${BLOCK_SIZES[@]}"; do
                        for optimizer in "${OPTIMIZERS[@]}"; do
                            if [ "$mode" = "FO" ]; then
                                for lr in "${LEARNING_RATES_ZO[@]}"; do
                                    experiments+=("$exp_id:$mode:$scope:$batch_size:$block_size:N/A:$lr:$optimizer:N/A:$model_size:N/A:N/A")
                                    exp_id=$((exp_id + 1))
                                done
                            elif [ "$mode" = "ZO" ]; then
                                for q in "${QUERY_BUDGETS[@]}"; do
                                    for lr in "${LEARNING_RATES_ZO[@]}"; do
                                        experiments+=("$exp_id:$mode:$scope:$batch_size:$block_size:$q:$lr:$optimizer:N/A:$model_size:N/A:N/A")
                                        exp_id=$((exp_id + 1))
                                    done
                                done
                            elif [ "$mode" = "Instruct" ]; then
                                for q in "${QUERY_BUDGETS[@]}"; do
                                    for lr in "${LEARNING_RATES_ZO[@]}"; do
                                        for bp_interval in "${BP_INTERVALS[@]}"; do
                                            for cos_target in "${INSTRUCT_COSINE_TARGETS[@]}"; do
                                                for noise_scale in "${INSTRUCT_NOISE_SCALES[@]}"; do
                                                    experiments+=("$exp_id:$mode:$scope:$batch_size:$block_size:$q:$lr:$optimizer:$bp_interval:$model_size:$cos_target:$noise_scale")
                                                    exp_id=$((exp_id + 1))
                                                done
                                            done
                                        done
                                    done
                                done
                            else
                                for q in "${QUERY_BUDGETS[@]}"; do
                                    for lr in "${LEARNING_RATES_ZO[@]}"; do
                                        for bp_interval in "${BP_INTERVALS[@]}"; do
                                            experiments+=("$exp_id:$mode:$scope:$batch_size:$block_size:$q:$lr:$optimizer:$bp_interval:$model_size:N/A:N/A")
                                            exp_id=$((exp_id + 1))
                                        done
                                    done
                                done
                            fi
                        done
                    done
                done
            done
        done
    done
    
    printf '%s\n' "${experiments[@]}"
}

# è¿è¡Œå•ä¸ªå®éªŒ
run_single_experiment() {
    local exp_config="$1"
    local gpu_id="$2"
    
    IFS=':' read -r exp_id mode scope batch_size block_size q lr optimizer bp_interval model_size cos_target noise_scale <<< "$exp_config"
    
    # å°† N/A æ›¿æ¢ä¸º NA ä»¥é¿å…æ–‡ä»¶è·¯å¾„é—®é¢˜
    local q_safe="${q//\//_}"
    local bp_safe="${bp_interval//\//_}"
    local cos_safe="${cos_target//\//_}"
    local noise_safe="${noise_scale//\//_}"
    local cos_label=""
    local noise_label=""
    if [ "$cos_target" != "N/A" ]; then
        cos_label="_ct${cos_safe}"
    fi
    if [ "$noise_scale" != "N/A" ]; then
        noise_label="_ns${noise_safe}"
    fi
    local exp_name="${mode}_${model_size}_${scope}_bs${batch_size}_blk${block_size}_q${q_safe}_bp${bp_safe}_opt${optimizer}_lr${lr}${cos_label}${noise_label}"
    local csv_file="${CSV_DIR}/${exp_name}.csv"
    local job_log="${JOB_LOG_DIR}/${exp_name}.log"
    local exp_log_dir="${EXPERIMENT_LOG_ROOT}/${exp_name}"
    local checkpoint_dir="${exp_log_dir}/checkpoint"
    local run_pid="${BASHPID:-$$}"
    
    mkdir -p "$exp_log_dir"
    echo -e "${YELLOW}ğŸ“Š Starting experiment: $exp_name (GPU: $gpu_id, PID: $run_pid)${NC}" | tee -a "$job_log"
    
    # æ„å»ºå‘½ä»¤
    local cmd="python reproduce_zo_paper.py"
    cmd="$cmd --mode $mode"
    cmd="$cmd --scope $scope"
    cmd="$cmd --batch_size $batch_size"
    cmd="$cmd --block_size $block_size"
    cmd="$cmd --learning_rate $lr"
    cmd="$cmd --optimizer $optimizer"
    cmd="$cmd --epochs $EPOCHS"
    cmd="$cmd --csv_file $csv_file"
    cmd="$cmd --log_interval $LOG_INTERVAL"
    cmd="$cmd --run_name $exp_name"
    cmd="$cmd --log_dir $exp_log_dir"
    cmd="$cmd --checkpoint_dir $checkpoint_dir"
    
    # æ³¨æ„: æ¨¡å‹å’Œæ•°æ®é›†é…ç½®ç›®å‰åœ¨Pythonè„šæœ¬ä¸­ç¡¬ç¼–ç 
    # å¦‚éœ€ä½¿ç”¨ä¸åŒé…ç½®ï¼Œè¯·ç›´æ¥ä¿®æ”¹ reproduce_zo_paper.py ä¸­çš„é…ç½®
    cmd="$cmd --model_size $model_size"
    cmd="$cmd --dataset $DATASET"
    if [ -n "$MAX_SAMPLES" ]; then
        cmd="$cmd --max_samples $MAX_SAMPLES"
    fi
    
    # BPæ•°æ®é›†é…ç½®ï¼ˆç”¨äºCalibrate/Instructæ¨¡å¼ï¼‰
    if [ -n "$BP_DATASET" ]; then
        cmd="$cmd --bp_dataset $BP_DATASET"
    fi
    if [ -n "$BP_MAX_SAMPLES" ]; then
        cmd="$cmd --bp_max_samples $BP_MAX_SAMPLES"
    fi
    
    if [[ "$mode" == "ZO" || "$mode" == "Calibrate" || "$mode" == "Instruct" ]] && [ "$q" != "N/A" ]; then
        cmd="$cmd --query_budget_q $q"
    fi
    if [ "$bp_interval" != "N/A" ]; then
        cmd="$cmd --bp_interval $bp_interval"
    fi
    if [ "$mode" = "Instruct" ] && [ "$cos_target" != "N/A" ]; then
        cmd="$cmd --instruct_cosine_target $cos_target"
    fi
    if [ "$mode" = "Instruct" ] && [ "$noise_scale" != "N/A" ]; then
        cmd="$cmd --instruct_noise_scale $noise_scale"
    fi
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°
    if [ "$USE_LR_SCHEDULER" = true ]; then
        cmd="$cmd --use_lr_scheduler"
        cmd="$cmd --warmup_steps $WARMUP_STEPS"
        cmd="$cmd --min_lr $MIN_LR"
    fi
    
    # æ¢¯åº¦ç´¯ç§¯å‚æ•°ï¼ˆä»…FOæ¨¡å¼ï¼‰
    if [ "$mode" = "FO" ] && [ "$GRADIENT_ACCUMULATION_STEPS" -gt 1 ]; then
        cmd="$cmd --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS"
    fi
    
    # è®¾ç½®GPUç¯å¢ƒå˜é‡
    if [ "$gpu_id" != "cpu" ]; then
        export CUDA_VISIBLE_DEVICES="$gpu_id"
    else
        unset CUDA_VISIBLE_DEVICES
    fi
    
    echo "Command: $cmd" >> "$job_log"
    echo "GPU: $gpu_id" >> "$job_log"
    echo "Shell PID: $run_pid" >> "$job_log"
    echo "CSV file: $csv_file" >> "$job_log"
    echo "Experiment log dir: $exp_log_dir" >> "$job_log"
    echo "Checkpoint dir: $checkpoint_dir" >> "$job_log"
    if [ "$mode" = "Instruct" ]; then
        echo "Instruct cosine target: $cos_target" >> "$job_log"
        echo "Instruct noise scale: $noise_scale" >> "$job_log"
    fi
    echo "Start time: $(date)" >> "$job_log"
    echo "----------------------------------------" >> "$job_log"
    
    # è¿è¡Œå®éªŒ
    eval $cmd >> "$job_log" 2>&1 &
    local child_pid=$!
    echo "Command PID: $child_pid" | tee -a "$job_log"
    
    # è®°å½• PID åˆ°æ–‡ä»¶å’Œå…¨å±€æ•°ç»„
    echo "$child_pid" >> "$PID_FILE"
    ALL_CHILD_PIDS+=("$child_pid")
    
    # è®°å½•åˆ°çŠ¶æ€æ–‡ä»¶
    echo "PID_${child_pid}=${exp_name}" >> "$STATUS_FILE"

    # ç­‰å¾…å­è¿›ç¨‹å®Œæˆï¼ˆä¸ç®¡æˆåŠŸè¿˜æ˜¯å¤±è´¥éƒ½ç»§ç»­ï¼‰
    wait $child_pid 2>/dev/null
    local exit_code=$?

    if [ $exit_code -eq 0 ]; then
        echo -e "${GREEN}âœ… Experiment $exp_name completed successfully${NC}" | tee -a "$job_log"
        echo "End time: $(date)" >> "$job_log"
        echo "SUCCESS" >> "$job_log"
    else
        # å­è¿›ç¨‹å¤±è´¥ä¸å½±å“å…¶ä»–è¿›ç¨‹ï¼Œåªè®°å½•å¤±è´¥ä¿¡æ¯
        echo -e "${RED}âŒ Experiment $exp_name failed with exit code $exit_code${NC}" | tee -a "$job_log"
        echo "End time: $(date)" >> "$job_log"
        echo "FAILED (exit code: $exit_code)" >> "$job_log"
        # æ³¨æ„ï¼šè¿™é‡Œä¸è¦ exit æˆ– return éé›¶å€¼ï¼Œè®©å…¶ä»–å®éªŒç»§ç»­è¿è¡Œ
    fi
    
    # ä»çŠ¶æ€æ–‡ä»¶ä¸­ç§»é™¤
    sed -i "/^PID_${child_pid}=/d" "$STATUS_FILE" 2>/dev/null || true

    # æ€»æ˜¯è¿”å› 0ï¼Œå•ä¸ªå®éªŒå¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹
    return 0
}

# å¹¶è¡Œæ‰§è¡Œå®éªŒ
run_parallel_experiments() {
    local experiments=($(generate_experiments))
    local total_experiments=${#experiments[@]}
    local completed=0
    local successful=0
    local failed=0
    
    echo -e "${BLUE}ğŸ“‹ Generated $total_experiments experiments${NC}"
    echo ""
    
    # å°†GPU IDè½¬æ¢ä¸ºæ•°ç»„ï¼ˆæ”¯æŒé€—å·å’Œç©ºæ ¼åˆ†éš”ï¼‰
    if [[ "$GPU_IDS" == *","* ]]; then
        IFS=',' read -ra GPU_ARRAY <<< "$GPU_IDS"
    else
        IFS=' ' read -ra GPU_ARRAY <<< "$GPU_IDS"
    fi
    local gpu_count=${#GPU_ARRAY[@]}
    local gpu_index=0
    
    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
    local job_queue=()
    local running_jobs=()
    
    # åˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—
    for exp in "${experiments[@]}"; do
        job_queue+=("$exp")
    done
    
    echo -e "${BLUE}ğŸš€ Starting parallel execution...${NC}"
    echo ""
    
    # ä¸»å¾ªç¯ï¼šç®¡ç†å¹¶è¡Œä»»åŠ¡
    while [ $completed -lt $total_experiments ]; do
        # å¯åŠ¨æ–°ä»»åŠ¡ï¼ˆå¦‚æœé˜Ÿåˆ—ä¸ä¸ºç©ºä¸”æœªè¾¾åˆ°æœ€å¤§å¹¶è¡Œæ•°ï¼‰
        while [ ${#running_jobs[@]} -lt $MAX_PARALLEL_JOBS ] && [ ${#job_queue[@]} -gt 0 ]; do
            local exp="${job_queue[0]}"
            job_queue=("${job_queue[@]:1}")  # ç§»é™¤ç¬¬ä¸€ä¸ªå…ƒç´ 
            
            local gpu_id="${GPU_ARRAY[$gpu_index]}"
            gpu_index=$(((gpu_index + 1) % gpu_count))
            
            # åœ¨åå°è¿è¡Œå®éªŒ
            run_single_experiment "$exp" "$gpu_id" &
            local pid=$!
            running_jobs+=("$pid:$exp:$gpu_id")
            
            echo -e "${PURPLE}ğŸ”„ Started job $pid for experiment $exp on GPU $gpu_id${NC}"
        done
        
        # æ£€æŸ¥å®Œæˆçš„ä»»åŠ¡
        local new_running_jobs=()
        for job in "${running_jobs[@]}"; do
            IFS=':' read -r pid exp gpu_id <<< "$job"
            if kill -0 $pid 2>/dev/null; then
                # ä»»åŠ¡ä»åœ¨è¿è¡Œ
                new_running_jobs+=("$job")
            else
                # ä»»åŠ¡å·²å®Œæˆï¼ˆä¸ç®¡æˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼‰
                local exit_code=0
                # ä½¿ç”¨ wait è·å–é€€å‡ºç ï¼Œå³ä½¿å¤±è´¥ä¹Ÿä¸ä¸­æ–­
                wait $pid 2>/dev/null || exit_code=$?
                
                completed=$((completed + 1))
                
                if [ $exit_code -eq 0 ]; then
                    successful=$((successful + 1))
                else
                    # å•ä¸ªä»»åŠ¡å¤±è´¥ä¸å½±å“å…¶ä»–ä»»åŠ¡
                    failed=$((failed + 1))
                fi
                
                echo -e "${BLUE}ğŸ“Š Progress: $completed/$total_experiments completed (Success: $successful, Failed: $failed)${NC}"
            fi
        done
        running_jobs=("${new_running_jobs[@]}")
        
        # æ›´æ–°çŠ¶æ€æ–‡ä»¶
        echo "PROGRESS=$completed/$total_experiments" > "${STATUS_FILE}.tmp"
        echo "SUCCESS=$successful" >> "${STATUS_FILE}.tmp"
        echo "FAILED=$failed" >> "${STATUS_FILE}.tmp"
        echo "RUNNING=${#running_jobs[@]}" >> "${STATUS_FILE}.tmp"
        cat "$STATUS_FILE" >> "${STATUS_FILE}.tmp"
        mv "${STATUS_FILE}.tmp" "$STATUS_FILE"
        
        # çŸ­æš‚ç­‰å¾…
        sleep 2
    done
    
    # ç­‰å¾…æ‰€æœ‰å‰©ä½™ä»»åŠ¡å®Œæˆ
    if [ ${#running_jobs[@]} -gt 0 ]; then
        echo -e "${YELLOW}ç­‰å¾… ${#running_jobs[@]} ä¸ªå‰©ä½™ä»»åŠ¡å®Œæˆ...${NC}"
        for job in "${running_jobs[@]}"; do
            IFS=':' read -r pid exp gpu_id <<< "$job"
            local exit_code=0
            # ç­‰å¾…æ¯ä¸ªè¿›ç¨‹ï¼Œå³ä½¿å¤±è´¥ä¹Ÿç»§ç»­å¤„ç†å…¶ä»–è¿›ç¨‹
            wait $pid 2>/dev/null || exit_code=$?
            
            completed=$((completed + 1))
            
            if [ $exit_code -eq 0 ]; then
                successful=$((successful + 1))
            else
                # å•ä¸ªä»»åŠ¡å¤±è´¥ä¸å½±å“å…¶ä»–ä»»åŠ¡
                failed=$((failed + 1))
            fi
        done
    fi
    
    # å¯¼å‡ºç»“æœä¾›ä¸»å‡½æ•°ä½¿ç”¨
    echo "$successful" > "$TEMP_DIR/successful_count"
    echo "$failed" > "$TEMP_DIR/failed_count"
    echo "$total_experiments" > "$TEMP_DIR/total_count"
    
    echo ""
    echo -e "${GREEN}ğŸ‰ All experiments completed!${NC}"
    echo "Total: $total_experiments, Success: $successful, Failed: $failed"
}

# ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
generate_final_report() {
    local start_time=$1
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    local hours=$((duration / 3600))
    local minutes=$(((duration % 3600) / 60))
    local seconds=$((duration % 60))
    
    # ä»æ–‡ä»¶è¯»å–ç»“æœ
    local successful=0
    local failed=0
    local total_experiments=0
    
    if [ -f "$TEMP_DIR/successful_count" ]; then
        successful=$(cat "$TEMP_DIR/successful_count")
    fi
    if [ -f "$TEMP_DIR/failed_count" ]; then
        failed=$(cat "$TEMP_DIR/failed_count")
    fi
    if [ -f "$TEMP_DIR/total_count" ]; then
        total_experiments=$(cat "$TEMP_DIR/total_count")
    fi
    
    local success_rate=0
    if [ $total_experiments -gt 0 ]; then
        success_rate=$(( successful * 100 / total_experiments ))
    fi
    
    echo -e "${BLUE}ğŸ“‹ PARALLEL SWEEP SUMMARY REPORT${NC}"
    echo -e "${BLUE}=================================${NC}"
    echo "Max parallel jobs: $MAX_PARALLEL_JOBS"
    echo "GPU IDs used: $GPU_IDS"
    echo "Model sizes: ${MODEL_SIZES[*]}"
    echo "Dataset: $DATASET"
    if [ -n "$BP_DATASET" ]; then
        echo "BP Dataset: $BP_DATASET"
    fi
    echo "Instruct cosine targets: ${INSTRUCT_COSINE_TARGETS[*]}"
    echo "Instruct noise scales: ${INSTRUCT_NOISE_SCALES[*]}"
    echo "Total experiments: $total_experiments"
    echo -e "Successful: ${GREEN}$successful${NC}"
    echo -e "Failed: ${RED}$failed${NC}"
    echo "Success rate: ${success_rate}%"
    echo "Total time: ${hours}h ${minutes}m ${seconds}s"
    echo ""
    echo "Results directory: $RESULTS_DIR"
    echo "CSV logs directory: $CSV_DIR"
    echo "Job logs directory: $JOB_LOG_DIR"
    echo "Log file: $LOG_FILE"
    echo "PID file: $PID_FILE"
    echo "Status file: $STATUS_FILE"
    echo "Summary file: $SUMMARY_FILE"
    echo ""
    
    # ä¿å­˜åˆ°æ‘˜è¦æ–‡ä»¶
    {
        echo "PARALLEL SWEEP SUMMARY REPORT"
        echo "================================="
        echo "Timestamp: $(date)"
        echo "Max parallel jobs: $MAX_PARALLEL_JOBS"
        echo "GPU IDs used: $GPU_IDS"
        echo "Model sizes: ${MODEL_SIZES[*]}"
        echo "Dataset: $DATASET"
        if [ -n "$BP_DATASET" ]; then
            echo "BP Dataset: $BP_DATASET"
        fi
        echo "Instruct cosine targets: ${INSTRUCT_COSINE_TARGETS[*]}"
        echo "Instruct noise scales: ${INSTRUCT_NOISE_SCALES[*]}"
        echo "Total experiments: $total_experiments"
        echo "Successful: $successful"
        echo "Failed: $failed"
        echo "Success rate: ${success_rate}%"
        echo "Total time: ${hours}h ${minutes}m ${seconds}s"
        echo ""
        echo "Results directory: $RESULTS_DIR"
        echo "CSV logs directory: $CSV_DIR"
        echo "Job logs directory: $JOB_LOG_DIR"
        echo "Log file: $LOG_FILE"
        echo "PID file: $PID_FILE"
        echo "Status file: $STATUS_FILE"
    } > "$SUMMARY_FILE"
    
    # åˆ—å‡ºæ‰€æœ‰ç»“æœæ–‡ä»¶
    echo -e "${BLUE}ğŸ“ Generated Files:${NC}"
    echo "PNG plots:"
    ls -la "$RESULTS_DIR"/*.png 2>/dev/null | head -10 || echo "  No PNG files found"
    if [ $(ls -1 "$RESULTS_DIR"/*.png 2>/dev/null | wc -l) -gt 10 ]; then
        echo "  ... and $(($(ls -1 "$RESULTS_DIR"/*.png 2>/dev/null | wc -l) - 10)) more files"
    fi
    echo ""
    echo "CSV logs:"
    ls -la "$CSV_DIR"/*.csv 2>/dev/null | head -10 || echo "  No CSV files found"
    if [ $(ls -1 "$CSV_DIR"/*.csv 2>/dev/null | wc -l) -gt 10 ]; then
        echo "  ... and $(($(ls -1 "$CSV_DIR"/*.csv 2>/dev/null | wc -l) - 10)) more files"
    fi
    echo ""
}

# ä¸»ç¨‹åº
main() {
    local start_time=$(date +%s)
    
    # è®°å½•é…ç½®
    echo "Configuration:" >> "$LOG_FILE"
    echo "MODES: ${MODES[*]}" >> "$LOG_FILE"
    echo "SCOPES: ${SCOPES[*]}" >> "$LOG_FILE"
    echo "BATCH_SIZES: ${BATCH_SIZES[*]}" >> "$LOG_FILE"
    echo "QUERY_BUDGETS: ${QUERY_BUDGETS[*]}" >> "$LOG_FILE"
    echo "BP_INTERVALS: ${BP_INTERVALS[*]}" >> "$LOG_FILE"
    echo "LEARNING_RATES_ZO: ${LEARNING_RATES_ZO[*]}" >> "$LOG_FILE"
    echo "OPTIMIZERS: ${OPTIMIZERS[*]}" >> "$LOG_FILE"
    echo "EPOCHS: $EPOCHS" >> "$LOG_FILE"
    echo "MODEL_SIZES: ${MODEL_SIZES[*]}" >> "$LOG_FILE"
    echo "DATASET: $DATASET" >> "$LOG_FILE"
    echo "MAX_SAMPLES: ${MAX_SAMPLES:-auto}" >> "$LOG_FILE"
    echo "BP_DATASET: ${BP_DATASET:-same_as_main}" >> "$LOG_FILE"
    echo "BP_MAX_SAMPLES: ${BP_MAX_SAMPLES:-auto}" >> "$LOG_FILE"
    echo "INSTRUCT_COSINE_TARGETS: ${INSTRUCT_COSINE_TARGETS[*]}" >> "$LOG_FILE"
    echo "INSTRUCT_NOISE_SCALES: ${INSTRUCT_NOISE_SCALES[*]}" >> "$LOG_FILE"
    echo "MAX_PARALLEL_JOBS: $MAX_PARALLEL_JOBS" >> "$LOG_FILE"
    echo "GPU_IDS: $GPU_IDS" >> "$LOG_FILE"
    echo "=========================================" >> "$LOG_FILE"
    
    # è¿è¡Œå¹¶è¡Œå®éªŒï¼ˆä¸è¦å› ä¸ºå•ä¸ªå®éªŒå¤±è´¥è€Œä¸­æ–­ï¼‰
    run_parallel_experiments 2>&1 | tee -a "$LOG_FILE"
    local run_exit_code=${PIPESTATUS[0]}
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_final_report "$start_time" 2>&1 | tee -a "$LOG_FILE"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥çš„å®éªŒ
    local failed_count=0
    if [ -f "$TEMP_DIR/failed_count" ]; then
        failed_count=$(cat "$TEMP_DIR/failed_count")
    fi
    
    if [ $failed_count -eq 0 ]; then
        echo -e "${GREEN}ğŸ‰ Parallel sweep completed successfully! All experiments passed.${NC}"
    else
        echo -e "${YELLOW}âš ï¸  Parallel sweep completed. $failed_count experiment(s) failed.${NC}"
        echo -e "${YELLOW}    Check individual job logs in $JOB_LOG_DIR for details.${NC}"
    fi
    echo "Check the results in the $RESULTS_DIR and $CSV_DIR directories."
    echo "Detailed logs available in: $LOG_FILE"
    echo "PID tracking file: $PID_FILE"
    echo "Status file: $STATUS_FILE"
    
    # å³ä½¿æœ‰å¤±è´¥çš„å®éªŒï¼Œä¹Ÿè¿”å› 0ï¼ˆæ•´ä½“æµç¨‹æˆåŠŸå®Œæˆï¼‰
    # å¦‚æœéœ€è¦æ ¹æ®å¤±è´¥æ•°é‡è¿”å›éé›¶ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    # [ $failed_count -eq 0 ] && return 0 || return 1
    return 0
}

# è¿è¡Œä¸»ç¨‹åº
main "$@"
