#!/usr/bin/env python3
"""
æµ‹è¯•ZOæ–¹æ³•çš„å†…å­˜ä¼˜åŒ–å’Œå¹¶è¡Œè®¡ç®—åŠŸèƒ½

ç”¨æ³•:
    python test_zo_optimization.py
"""

import torch
import time
import tracemalloc
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from reproduce_zo_paper import zo_gradient_estimator
from cores.model import create_model
from transformers import AutoTokenizer
from torch.nn import CrossEntropyLoss


def measure_memory(func, *args, **kwargs):
    """æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶çš„å†…å­˜ä½¿ç”¨"""
    torch.cuda.empty_cache()
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()
        start_mem = torch.cuda.memory_allocated()
    
    tracemalloc.start()
    start_time = time.time()
    
    result = func(*args, **kwargs)
    
    elapsed_time = time.time() - start_time
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    
    if torch.cuda.is_available():
        end_mem = torch.cuda.memory_allocated()
        peak_mem = torch.cuda.max_memory_allocated()
        gpu_mem_used = (peak_mem - start_mem) / 1024**2  # MB
    else:
        gpu_mem_used = 0
    
    return result, elapsed_time, gpu_mem_used


def test_zo_memory_optimization():
    """æµ‹è¯•ZOæ–¹æ³•çš„å†…å­˜ä¼˜åŒ–"""
    print("=" * 70)
    print("æµ‹è¯• 1: ZOæ–¹æ³•å†…å­˜ä¼˜åŒ–éªŒè¯")
    print("=" * 70)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºå°æ¨¡å‹ç”¨äºæµ‹è¯•
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    
    model = create_model(model_size='20M', vocab_size=len(tokenizer)).to(device)
    loss_fn = CrossEntropyLoss()
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    batch_size = 4
    block_size = 128
    inputs = torch.randint(0, len(tokenizer), (batch_size, block_size)).to(device)
    labels = inputs.clone()
    
    trainable_params = [p for p in model.parameters() if p.requires_grad]
    
    print(f"\næ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in trainable_params):,}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}, åºåˆ—é•¿åº¦: {block_size}")
    
    # æµ‹è¯•ä¸åŒçš„qå€¼
    q_values = [1, 4, 10]
    
    for q in q_values:
        print(f"\næµ‹è¯• q={q} (é¡ºåºè®¡ç®—):")
        (grads, loss), elapsed, mem = measure_memory(
            zo_gradient_estimator,
            model, trainable_params, loss_fn, inputs, labels,
            q=q, epsilon=1e-3, device=device,
            parallel_q_computation=False
        )
        print(f"  æ—¶é—´: {elapsed:.3f}ç§’")
        print(f"  GPUå†…å­˜å¢é‡: {mem:.2f} MB")
        print(f"  æŸå¤±å€¼: {loss:.4f}")
        print(f"  æ¢¯åº¦èŒƒæ•°: {sum(torch.norm(g).item() for g in grads):.6f}")
    
    print("\nâœ… å†…å­˜ä¼˜åŒ–æµ‹è¯•å®Œæˆï¼")
    print("æ³¨æ„ï¼šä¼˜åŒ–ç‰ˆæœ¬é¿å…äº†å‚æ•°å…‹éš†ï¼Œå†…å­˜å¢é‡åº”è¯¥å¾ˆå°ã€‚")


def test_parallel_q_computation():
    """æµ‹è¯•å¹¶è¡ŒQå€¼è®¡ç®—"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 2: å¹¶è¡ŒQå€¼è®¡ç®—æ€§èƒ½å¯¹æ¯”")
    print("=" * 70)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    
    model = create_model(model_size='20M', vocab_size=len(tokenizer)).to(device)
    loss_fn = CrossEntropyLoss()
    
    batch_size = 4
    block_size = 128
    inputs = torch.randint(0, len(tokenizer), (batch_size, block_size)).to(device)
    labels = inputs.clone()
    
    trainable_params = [p for p in model.parameters() if p.requires_grad]
    
    # æµ‹è¯•ä¸åŒqå€¼ä¸‹çš„é¡ºåºvså¹¶è¡Œæ€§èƒ½
    q_values = [8, 16, 32]
    batch_sizes = [4, 8]
    
    for q in q_values:
        print(f"\næµ‹è¯• q={q}:")
        
        # é¡ºåºè®¡ç®—
        print(f"  é¡ºåºè®¡ç®—:")
        (grads_seq, loss_seq), time_seq, mem_seq = measure_memory(
            zo_gradient_estimator,
            model, trainable_params, loss_fn, inputs, labels,
            q=q, epsilon=1e-3, device=device,
            parallel_q_computation=False
        )
        print(f"    æ—¶é—´: {time_seq:.3f}ç§’")
        print(f"    å†…å­˜: {mem_seq:.2f} MB")
        print(f"    æŸå¤±: {loss_seq:.4f}")
        
        # å¹¶è¡Œè®¡ç®—ï¼ˆä¸åŒæ‰¹æ¬¡å¤§å°ï¼‰
        for batch_size_p in batch_sizes:
            print(f"  å¹¶è¡Œè®¡ç®— (batch_size={batch_size_p}):")
            (grads_par, loss_par), time_par, mem_par = measure_memory(
                zo_gradient_estimator,
                model, trainable_params, loss_fn, inputs, labels,
                q=q, epsilon=1e-3, device=device,
                parallel_q_computation=True,
                parallel_batch_size=batch_size_p
            )
            
            speedup = (time_seq / time_par - 1) * 100  # ç™¾åˆ†æ¯”
            print(f"    æ—¶é—´: {time_par:.3f}ç§’ (ç›¸æ¯”é¡ºåº: {speedup:+.1f}%)")
            print(f"    å†…å­˜: {mem_par:.2f} MB")
            print(f"    æŸå¤±: {loss_par:.4f}")
            
            # éªŒè¯ç»“æœä¸€è‡´æ€§ï¼ˆå…è®¸å°çš„æ•°å€¼è¯¯å·®ï¼‰
            loss_diff = abs(loss_seq - loss_par)
            if loss_diff < 1e-4:
                print(f"    âœ… ç»“æœéªŒè¯: ä¸€è‡´ (diff={loss_diff:.6f})")
            else:
                print(f"    âš ï¸  ç»“æœéªŒè¯: å¯èƒ½ä¸ä¸€è‡´ (diff={loss_diff:.6f})")
    
    print("\nâœ… å¹¶è¡Œè®¡ç®—æµ‹è¯•å®Œæˆï¼")


def test_backward_compatibility():
    """æµ‹è¯•å‘åå…¼å®¹æ€§"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• 3: å‘åå…¼å®¹æ€§éªŒè¯")
    print("=" * 70)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    
    model = create_model(model_size='20M', vocab_size=len(tokenizer)).to(device)
    loss_fn = CrossEntropyLoss()
    
    inputs = torch.randint(0, len(tokenizer), (2, 64)).to(device)
    labels = inputs.clone()
    
    trainable_params = [p for p in model.parameters() if p.requires_grad]
    
    print("\næµ‹è¯•ä¸å¸¦æ–°å‚æ•°çš„è°ƒç”¨ï¼ˆåº”è¯¥æ­£å¸¸å·¥ä½œï¼‰:")
    try:
        grads, loss = zo_gradient_estimator(
            model, trainable_params, loss_fn, inputs, labels,
            q=2, epsilon=1e-3, device=device
        )
        print(f"  âœ… æˆåŠŸ: loss={loss:.4f}")
    except Exception as e:
        print(f"  âŒ å¤±è´¥: {e}")
    
    print("\næµ‹è¯•å¸¦æ–°å‚æ•°çš„è°ƒç”¨:")
    try:
        grads, loss = zo_gradient_estimator(
            model, trainable_params, loss_fn, inputs, labels,
            q=2, epsilon=1e-3, device=device,
            parallel_q_computation=True,
            parallel_batch_size=2
        )
        print(f"  âœ… æˆåŠŸ: loss={loss:.4f}")
    except Exception as e:
        print(f"  âŒ å¤±è´¥: {e}")
    
    print("\nâœ… å‘åå…¼å®¹æ€§æµ‹è¯•å®Œæˆï¼")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n")
    print("â•”" + "â•" * 68 + "â•—")
    print("â•‘" + " " * 15 + "ZOæ–¹æ³•ä¼˜åŒ–åŠŸèƒ½æµ‹è¯•å¥—ä»¶" + " " * 15 + "â•‘")
    print("â•š" + "â•" * 68 + "â•")
    print()
    
    try:
        # æµ‹è¯•1: å†…å­˜ä¼˜åŒ–
        test_zo_memory_optimization()
        
        # æµ‹è¯•2: å¹¶è¡Œè®¡ç®—
        test_parallel_q_computation()
        
        # æµ‹è¯•3: å‘åå…¼å®¹æ€§
        test_backward_compatibility()
        
        print("\n" + "=" * 70)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("=" * 70)
        
        print("\næ€»ç»“:")
        print("1. âœ… å†…å­˜ä¼˜åŒ–: é¿å…å‚æ•°å…‹éš†ï¼Œå‡å°‘å†…å­˜å ç”¨")
        print("2. âœ… å¹¶è¡Œè®¡ç®—: æ‰¹é‡å¤„ç†Qå€¼ï¼Œæå‡è®¡ç®—æ•ˆç‡")
        print("3. âœ… å‘åå…¼å®¹: ä¿æŒåŸæœ‰APIå…¼å®¹æ€§")
        
        print("\nä½¿ç”¨å»ºè®®:")
        print("- æ‰€æœ‰ZOæ–¹æ³•è‡ªåŠ¨äº«å—å†…å­˜ä¼˜åŒ–")
        print("- qå€¼è¾ƒå¤§æ—¶(>8)ï¼Œå»ºè®®å¯ç”¨å¹¶è¡Œè®¡ç®—")
        print("- å¹¶è¡Œæ‰¹æ¬¡å¤§å°æ¨èå€¼: 4-8")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯:")
        print(f"   {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

